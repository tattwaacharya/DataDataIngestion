{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tattwaacharya/DataDataIngestion/blob/master/M5_AST_02_Intro_to_PySpark_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr6L0U0ahbzr"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 2: Intro to PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ-wAYVIhbzu"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnSM9v-nhbzv"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* interact with Spark using python\n",
        "* understand Spark dataframes\n",
        "* implement linear regression using PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3T1Tj2fhbzw"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8LVD5gchbzw"
      },
      "source": [
        "The dataset chosen for this assignment is [Ecommerce customers](https://www.kaggle.com/srolka/ecommerce-customers). The dataset is made up of 500 records and 8 columns. It has customer information, such as e-mail, address, and their color avatar. Then it also has numerical value columns.\n",
        "\n",
        "* Avg Session Length: Average session of in-store style advice sessions\n",
        "* Time on App: Average time spent on App in minutes\n",
        "* Time on Website: Average time spent on Website in minutes\n",
        "* Length of Membership: How many years the customer has been a member.\n",
        "* Yearly Amount Spent\n",
        "\n",
        "Here, we will be using the first four features to perform linear regression using spark and predict Yearly Amount Spent by each customer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS6X9xQQhbzx"
      },
      "source": [
        "### Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLUYa2jkhbzx"
      },
      "source": [
        "**Why do we need Spark?**\n",
        "\n",
        "Spark is one of the latest technologies being used to quickly and easily handle Big Data. Spark is an open-source distributed computing framework that promises a clean and pleasurable experience similar to that of Pandas, while scaling to large data sets via a distributed architecture under the hood.\n",
        "\n",
        "Apache Spark is a powerful cluster computing engine, therefore it is designed for fast computation of big data. Spark runs on Memory (RAM), and that makes the processing much faster than on Disk. It includes \"MLlib\" library to perform Machine Learning tasks using the Spark framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP0oxBKKhbzx"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-MIetu-hbzy"
      },
      "source": [
        "Apache Spark is known as a fast, easy to use and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. It’s well-known for its speed, ease of use, generality and the ability to run virtually everywhere. And even though Spark is one of the most asked tools for data engineers, also data scientists can benefit from Spark when doing exploratory data analysis, feature extraction, supervised learning and model evaluation.\n",
        "\n",
        "Spark is a platform for cluster computing that lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
        "\n",
        "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M5_AST_02_Intro_to_PySpark_A\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/ecommerce_customers_.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxCv5teXYANs"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB8vMNkUYANt"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dwQCqpwhbzy"
      },
      "source": [
        "### PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYVoE0nThbzz"
      },
      "source": [
        "PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment. PySpark supports most of Spark’s features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core.\n",
        "\n",
        "<figure>\n",
        "<img src='https://cdn.iisc.talentsprint.com/CDS/Images/pyspark_components.png' width = 700 px/>\n",
        "</figure>\n",
        "\n",
        "**Spark SQL and DataFrame**\n",
        "\n",
        "Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrame and can also act as distributed SQL query engine.\n",
        "\n",
        "**Streaming**\n",
        "\n",
        "Running on top of Spark, the streaming feature in Apache Spark enables powerful interactive and analytical applications across both streaming and historical data, while inheriting Spark’s ease of use and fault tolerance characteristics.\n",
        "\n",
        "**MLlib**\n",
        "\n",
        "Built on top of Spark, MLlib is a scalable machine learning library that provides a uniform set of high-level APIs that help users create and tune practical machine learning pipelines.\n",
        "\n",
        "**Spark Core**\n",
        "\n",
        "Spark Core is the underlying general execution engine for the Spark platform that all other functionality is built on top of. It provides an RDD (Resilient Distributed Dataset) and in-memory computing capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuVyxDSAhbzz"
      },
      "source": [
        "#### Install PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpbyJFSZhbzz"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-JqQmojhbz0"
      },
      "source": [
        "#### Start a Spark Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vju8S9ZWhbz0"
      },
      "source": [
        "Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0. It provides a way to interact with various spark’s functionality with a lesser number of constructs. Instead of having spark context, hive context, SQL context, now everything is encapsulated in a Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdlUo9HFhbz0"
      },
      "source": [
        "# Start spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('LinearRegression').getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryDcIBTYhbz1"
      },
      "source": [
        "### Data Processing using Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfm30-_ghbz1"
      },
      "source": [
        "#### Loading data into PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YyXV9pthbz1"
      },
      "source": [
        "To load the dataset we will use the `read.csv` module.  The `inferSchema` parameter provided will enable Spark to automatically determine the data type for each column. Also, `header` and `sep` parameters are given as the dataset contains header, and values are separated using vertical bar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g1oX-k0hbz2"
      },
      "source": [
        "df = spark.read.csv(\"ecommerce_customers_.csv\", sep = \"|\", header=True, inferSchema = True)           # creating spark data frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe2TqI_7hbz3"
      },
      "source": [
        "#### Data exploration with PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MOtesQghbz3"
      },
      "source": [
        "* Display data types of dataframe columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_xCliuMhbz4"
      },
      "source": [
        "# Print the data types\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfpmyItPhbz4"
      },
      "source": [
        "* Display column details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOnGCTXZhbz4"
      },
      "source": [
        "# Print the Schema of the DataFrame\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRI0Hi6phbz5"
      },
      "source": [
        "* Display rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4pNnsfwhbz5"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OicqJUYfhbz5"
      },
      "source": [
        "* Display total number of rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVMYaotJhbz5"
      },
      "source": [
        "# YOUR CODE HERE to count the rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJLKs8i3hbz6"
      },
      "source": [
        "* Display column labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CbPm_VUhbz6"
      },
      "source": [
        "# YOUR CODE HERE to display column labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbp-QZ3fhbz6"
      },
      "source": [
        "* Display specific columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uicyXLDAhbz6"
      },
      "source": [
        "columns = [\"Email\",\"Time on App\",\"Time on Website\"]\n",
        "df.select(columns).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyJyBM-Chbz6"
      },
      "source": [
        "* Display the statistics of dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEcVV2Achbz7"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ncz4X2hbz7"
      },
      "source": [
        "* Display total distinct values in *Avatar* column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3IUdO7Ohbz7"
      },
      "source": [
        "# Distinct value count\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDwuxwbfhbz8"
      },
      "source": [
        "* Display count of distinct values in *Avatar* column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up-ef7XNhbz8"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5bUrhKkhbz8"
      },
      "source": [
        "* Plot the count of distinct values in *Avatar* column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY39dnmohbz8"
      },
      "source": [
        "DF = df.groupby('Avatar').count().sort(\"count\", ascending= False)\n",
        "DF.show(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy5htH2Rhbz9"
      },
      "source": [
        "plt.figure(figsize= (24,4))\n",
        "x = DF.toPandas()['Avatar']\n",
        "# YOUR CODE HERE to create y\n",
        "sns.barplot(x, y)\n",
        "plt.xticks(rotation= 90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwFMYWqKhbz9"
      },
      "source": [
        "* Display average time spent on app by users having different *Avatar*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wi64y0Qhbz9"
      },
      "source": [
        "df.groupby('Avatar').avg().select(['Avatar', 'avg(Time on App)']).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcCKpLhYhbz-"
      },
      "source": [
        "* Display the records where average time spent on website by user is greater than 37 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLD3pmpQhbz-"
      },
      "source": [
        "df.filter(df['Time on Website'] > 37).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nB82Y8vhbz-"
      },
      "source": [
        "* Display the minimum Yearly Amount Spent where average time spent on website by user is greater than 39 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7caT8Lvhbz_"
      },
      "source": [
        "from pyspark.sql.functions import col, min\n",
        "df.filter(col('Time on Website')>39).agg(min('Yearly Amount Spent')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1hUIVRAhbz_"
      },
      "source": [
        "* Display the records where average time spent on app by user is greater than 12 minutes and average time spent on website is smaller than 37 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ-Hib-Bhbz_"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LORVLYUMhb0A"
      },
      "source": [
        "To know more about other `pyspark.sql.functions` operation click [here](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI55eNtchb0A"
      },
      "source": [
        "### Linear Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SQI5Rq3hb0A"
      },
      "source": [
        "Linear Regression model is one of the oldest and widely used machine learning approach which assumes a relationship between dependent and independent variables. It consists of the best fitting line through the scattered points on the graph and this best fitting line is known as the regression line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij78EczBhb0B"
      },
      "source": [
        "#### Setting Up DataFrame for Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exKHqPx4hb0B"
      },
      "source": [
        "For Spark to accept the data, it needs to be in the form of two columns (\"labels\", \"features\")\n",
        "\n",
        "* Features are data points of all the attributes to be used for prediction\n",
        "* Labels are output for each data point\n",
        "* We will be predicting Label from Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCZCS-R4hb0B"
      },
      "source": [
        "For the linear regression model, we need to import two modules from Pyspark i.e. Vector Assembler and Linear Regression. Vector Assembler is a transformer that assembles all the features into one vector from multiple columns that contain type double.\n",
        "\n",
        "To know more about vector assembler click [here](https://spark.apache.org/docs/2.1.0/ml-features.html#vectorassembler)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gHmgn46hb0C"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S26ufGiZhb0C"
      },
      "source": [
        "assembler = VectorAssembler(\n",
        "                            inputCols= [\"Avg Session Length\", \"Time on App\", \"Time on Website\",'Length of Membership'],\n",
        "                            outputCol= \"features\")       # features is the name of output columns which combines all the columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9umLjr7hb0C"
      },
      "source": [
        "output = assembler.transform(df)            # A new column 'features' will be created along with the existing columns\n",
        "                                            # features column will include all the values combined in one list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOxDfY4uhb0D"
      },
      "source": [
        "# YOUR CODE HERE to display first 10 rows of output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l3IvdYNhb0D"
      },
      "source": [
        "output.select(\"features\").show(10, truncate= False)          # displays only the features column (which includes all other column values in a list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsMl8o7Yhb0E"
      },
      "source": [
        "# Complete dataset is represented in 2 columns\n",
        "final_data = output.select(\"features\",'Yearly Amount Spent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dyT5lLehb0E"
      },
      "source": [
        "#### Splitting the data into Training and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cznY4s_Ahb0F"
      },
      "source": [
        "# Splitting the data in Train and Test set(70% training data, 30% testing data)\n",
        "# YOUR CODE HERE to create train_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQzau_Pvhb0F"
      },
      "source": [
        "train_data.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrdTvsmVhb0F"
      },
      "source": [
        "# YOUR CODE HERE to describe test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW62eB6Hhb0G"
      },
      "source": [
        "#### Create a Linear Regression Model object and fit on train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86V5LekMhb0G"
      },
      "source": [
        "regressor = LinearRegression(featuresCol=\"features\", labelCol=\"Yearly Amount Spent\")\n",
        "\n",
        "# Learn to fit the model from training set\n",
        "# YOUR CODE HERE to fit regressor on train_data and create model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVH4X80nhb0G"
      },
      "source": [
        "#### Predicting the Test set results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCrfunGbhb0G"
      },
      "source": [
        "predict = model.transform(test_data)\n",
        "\n",
        "predict.select(predict.columns[:]).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHlcdOtThb0G"
      },
      "source": [
        "#### Evaluating Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41ftFH8Ohb0H"
      },
      "source": [
        "# YOUR CODE HERE to create metrics                            # Using evaluate method we can verify our model's performance\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcCSeWV3hb0H"
      },
      "source": [
        "To know more about other operations in pyspark click [here](https://cdn.iisc.talentsprint.com/CDS/cheatSheet_pyspark.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "# @title In the above given spark dataframe (df), what is the total number of records where average time spent on app by user is greater than 13 minutes? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\",\"90\",\"91\", \"92\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}